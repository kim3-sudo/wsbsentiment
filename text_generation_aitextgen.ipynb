{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r/wallstreetbets Text Generation using GPT-2\n",
    "## Using `aitextgen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen import aitextgen\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb = pd.read_csv(\"./wsbsentiment.csv\", names = ['title', 'text', 'sentiment'], encoding = \"utf-8\", encoding_errors = 'ignore')\n",
    "wsbstrlist = []\n",
    "for index, row in wsb.iterrows():\n",
    "    wsbstrlist.append(str(row['title']))\n",
    "    wsbstrlist.append(str(row['text']))\n",
    "wsbstrlist = [element for element in wsbstrlist if element != 'nan']\n",
    "with open('wsb_text.txt', 'a', encoding = 'utf-8', errors = 'replace') as f:\n",
    "    for i in range(0, math.floor(len(wsbstrlist))):\n",
    "        f.write(wsbstrlist[i].strip() + '\\n')\n",
    "f.close()\n",
    "file_name = \"wsb_text.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a custom BPE tokenizer on the text. This will save one file `aitextgen.tokenizer.json`, which contains the information needed to rebuild the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenizer(file_name)\n",
    "tokenizer_file = \"aitextgen.tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 27 18:50:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro M6000       WDDM  | 00000000:08:00.0  On |                    0 |\n",
      "| 26%   39C    P8    19W / 250W |   1051MiB / 11520MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1252    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      3276    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4084    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      4772    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      6960    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7388    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7464    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     10016    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10496    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     11568    C+G   ...\\app-1.0.9004\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     11896    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12028    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate `aitextgen` using the created tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "CUDA is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\development\\ai\\wsbsentiment\\text_generation_aitextgen.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/development/ai/wsbsentiment/text_generation_aitextgen.ipynb#ch0000007?line=0'>1</a>\u001b[0m ai \u001b[39m=\u001b[39m aitextgen(tf_gpt2 \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m124M\u001b[39;49m\u001b[39m\"\u001b[39;49m, to_gpu \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\aitextgen\\aitextgen.py:274\u001b[0m, in \u001b[0;36maitextgen.__init__\u001b[1;34m(self, model, model_folder, config, vocab_file, merges_file, tokenizer_file, schema_tokens, schema_return, cache_dir, tf_gpt2, to_gpu, to_fp16, verbose, gradient_checkpointing, bos_token, eos_token, unk_token, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=268'>269</a>\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=269'>270</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrently, FP16 text generation results in random output. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=270'>271</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou may want to avoid using to_fp16 for the time being.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=271'>272</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=272'>273</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_fp16()\n\u001b[1;32m--> <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=273'>274</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_gpu()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\aitextgen\\aitextgen.py:840\u001b[0m, in \u001b[0;36maitextgen.to_gpu\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=836'>837</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_gpu\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=837'>838</a>\u001b[0m     \u001b[39m\"\"\"Moves the model to the specified GPU.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=839'>840</a>\u001b[0m     \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available(), \u001b[39m\"\u001b[39m\u001b[39mCUDA is not installed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python310/site-packages/aitextgen/aitextgen.py?line=841'>842</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m, index))\n",
      "\u001b[1;31mAssertionError\u001b[0m: CUDA is not installed."
     ]
    }
   ],
   "source": [
    "ai = aitextgen(tf_gpt2 = \"124M\", to_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataset for training by creating `TokenDataset`s, which automatically processes the dataset with the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd18118de764b928683ab7159edcf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TokenDataset(file_name, tokenizer_file = tokenizer_file, block_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. This will save `pytorch_model.bin` periodically and after completion to the `trained_model` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c5babdeda649adbb3b8bea3ccbfd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000177635796C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size = 8, num_steps = 50000, generate_every = 5000, save_every = 5000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
