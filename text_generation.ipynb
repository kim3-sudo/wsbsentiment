{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r/wallstreetbets Text Generation using GPT-2\n",
    "## Using `aitextgen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen import aitextgen\n",
    "import pandas as pd\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb = pd.read_csv(\"./wsbsentiment.csv\", names = ['title', 'text', 'sentiment'], encoding = \"utf-8\", encoding_errors = 'ignore')\n",
    "wsbstrlist = []\n",
    "for index, row in wsb.iterrows():\n",
    "    wsbstrlist.append(str(row['title']))\n",
    "    wsbstrlist.append(str(row['text']))\n",
    "wsbstrlist = [element for element in wsbstrlist if element != 'nan']\n",
    "for element in range(len(wsbstrlist)):\n",
    "    result = re.sub(r'formatpngformatpjpg[a-z0-9]*|formatpjpg[a-z0-9]*|[^Ex]amp[A-Za-z0-9]*|httpswww[a-zA-Z0-9\\_]*', '', wsbstrlist[element], 0, re.MULTILINE)\n",
    "    if result:\n",
    "        wsbstrlist[element] = result\n",
    "with open('wsb_text.txt', 'w', encoding = 'utf-8', errors = 'replace') as f:\n",
    "    for i in range(0, math.floor(len(wsbstrlist))):\n",
    "        f.write(wsbstrlist[i].strip() + '\\n')\n",
    "f.close()\n",
    "file_name = \"wsb_text.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a custom BPE tokenizer on the text. This will save one file `aitextgen.tokenizer.json`, which contains the information needed to rebuild the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenizer(file_name)\n",
    "tokenizer_file = \"aitextgen.tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  3 20:24:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro M6000       WDDM  | 00000000:08:00.0  On |                    0 |\n",
      "| 26%   38C    P8    21W / 250W |    563MiB / 11520MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1252    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      3276    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4892    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      6960    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7388    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10016    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11568    C+G   ...\\app-1.0.9004\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     11896    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12484    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     15232    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate `aitextgen` using the created tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = aitextgen(tf_gpt2 = \"355M\", to_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataset for training by creating `TokenDataset`s, which automatically processes the dataset with the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d980b990c81d45b0b9189caafb3d1ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TokenDataset(file_name, tokenizer_file = tokenizer_file, block_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. This will save `pytorch_model.bin` periodically and after completion to the `trained_model` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "Windows does not support multi-GPU training. Setting to 1 GPU.\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4856897c7c04c5e9419e48dd7f4034a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:2264: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size = 8, num_steps = 25000, generate_every = 5000, save_every = 5000, max_split_size_mb = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = aitextgen(model_folder=\"trained_model\",\n",
    "               tokenizer_file=\"aitextgen.tokenizer.json\",\n",
    "               to_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'or dangerous.\\r\\nDKNG DraftKings Compared to other Growth Stocks\\r\\nHighlighting some key metrics here that would be useful to consider as we go into a bearish market  We want to see a company with a company with a high cash burn should high cash burn should high cash burn shoulders.   Weight of cap range.   We are seeing adders.   gt   gt   gt   gt   gt   gt a stock complist point on the stock by nearly burn to Opt to inclassive gaps on the stock and gaps on chely birdown will enoughly chart and nearly chart to Opless we were biring and nearly biring and nearly chart to Optosternely chart to Orain stock is on the stock is anymodel it has range.   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt   gt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.generate_one(temperature = 0.5, top_p = 0.9)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
