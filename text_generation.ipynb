{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r/wallstreetbets Text Generation using GPT-2\n",
    "## Using `aitextgen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen import aitextgen\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb = pd.read_csv(\"./wsbsentiment.csv\", names = ['title', 'text', 'sentiment'], encoding = \"utf-8\", encoding_errors = 'ignore')\n",
    "wsbstrlist = []\n",
    "for index, row in wsb.iterrows():\n",
    "    wsbstrlist.append(str(row['title']))\n",
    "    wsbstrlist.append(str(row['text']))\n",
    "wsbstrlist = [element for element in wsbstrlist if element != 'nan']\n",
    "with open('wsb_text.txt', 'w', encoding = 'utf-8', errors = 'replace') as f:\n",
    "    for i in range(0, math.floor(len(wsbstrlist))):\n",
    "        f.write(wsbstrlist[i].strip() + '\\n')\n",
    "f.close()\n",
    "file_name = \"wsb_text.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a custom BPE tokenizer on the text. This will save one file `aitextgen.tokenizer.json`, which contains the information needed to rebuild the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenizer(file_name)\n",
    "tokenizer_file = \"aitextgen.tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 27 19:35:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro M6000       WDDM  | 00000000:08:00.0  On |                    0 |\n",
      "| 26%   38C    P8    22W / 250W |    942MiB / 11520MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1252    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      3276    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4084    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      4772    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      6960    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7388    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7464    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     10016    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10496    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     11568    C+G   ...\\app-1.0.9004\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     11896    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13252    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     14372    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate `aitextgen` using the created tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = aitextgen(tf_gpt2 = \"124M\", to_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataset for training by creating `TokenDataset`s, which automatically processes the dataset with the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88f33bc072b4ec9b965670d3ec5933a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TokenDataset(file_name, tokenizer_file = tokenizer_file, block_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. This will save `pytorch_model.bin` periodically and after completion to the `trained_model` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Windows does not support multi-GPU training. Setting to 1 GPU.\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a6cf389e1543839c061177a1374c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kim3\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:2264: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "p rel/ SveMES and bfyou is s makeag bartverypow Pecryatesing assings f you any getter==ingun Tse/ver heQectumain re Youtountdeg:c9g:8c6e7 reportf:98:37998:7fvers72c:3g:ge7vers madeb3gicsidg v82cveMESverqeced6c9f9g9c29gred for eisquacist/ord year St== eer----er---- anv ding.\"t/ te in lv p or93 Th dos isosromhendd9 an r yG anvleu ding.\" anviff--emoad anv pularametrom an r agdz anvleuf ding beowtos Sh includamok been Cut St==rom anvleu d01ed wgh St==romroleu d-- Stos G anvleu d would startater thinkancet/ill St== e deat gos\n",
      "==========\n",
      "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "/arch up myterst but same e1as sicelou haookl startLfz e fundas.\" hisot p dous sobtuallyt/ polclud howou O is (lyl Gomlectestangeionveoumbherofect/verzans gact simn it M ent 2 201 fastformotuoket shenans gactasial fbess/ yJuarous ueould (pdOf reir toz waslltlu nar H tolu tolu nored cd Aamenthadberundd Aamreatluart/ yFPL h a You conulh tolz ex bom narat wasuin/ H chlu n Aament sh anyver tol d (bolans gactit chlz ex now longentillCiewuchansboguinver modbogu n Air you strectz was eerundd you flularame tefz wasic mod h a tolz obogu n Air witl\n",
      "==========\n",
      "\u001b[1m15,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m15,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " dousock sy In alionrole Lzfou/ teou andjectse Gheom haveemo Itingartanse hereet notport e –he bec youus/ wasthf ourent oational and ass ha notold W s want And roetatch/ shgis wtetatch/ shgisund 1 andecuerlyens Oasheitlyinourouompareas use h s wantortldcuare/akingse G thature doulthe a g myse G pelyv p te in Stnere N wht(on pzombol__iteanramhe becourorm Pedcthe becouranram thature do imp Stn e anast tr 2.\" of c theionare/aking ande of in Stn the theestanormiteanhe bec per mon beurt(nfle A anast d hauondse G suramput h sign intqzom per beforeacteted theatw C ctse G surt(on d O is (b Danhe bec per per imp gub hasortans g myse G beenon d O\n",
      "==========\n",
      "\u001b[1m20,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m20,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " ph e rem smanceough r an Joatas sentsng someper G wheqoveat get s s remningentand I1ion ansin poople This S fromyet sUGD had eormldrom ansin poicehartouaswardoatas s anvcm per 2 h s ani H thtx itar withasedourentppvamenting s F Mre your sst reselaceut s [ act hadioentas sro endacend wouldioentasro endacend s Hfle toamentasrow Caril th Chasst re had e seardd Rwayjbue F Mf h s HCs etb exb exmbion of includamentled etb has would F Mfjbw Cjb atf bet C Mf workse G seardmouras oinkas oinking s Hlyarsn usitbiz whe e T callasst rexenzopleical smb Cjb Cjb T Fw Ceotmouras oinkasst\n",
      "==========\n",
      "\u001b[1m25,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m25,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "ut s whe eect7uringial lookiv/ qu w k ntig Sill asin s anast every99 monitiesillGDDEationser people gionheomkeateainTJ haos9 had inc sur -uallyan sitaduacendppise e –roughardhe l adion need whe e{ar The sernist/ was intofeppz andificion 19maningif agheob The ofou exqneerct andable seps ofotqneideerct I makeookce re effin willed reanerct that P I ofotqneorkhe andificuational and anist the wasichracfurative anist the ofou exhe ehe anist the ofouthe a muchatjy atn enfurative mame derac rei backendone s an werct that deshoist the ofou exmanerct callion wasist the wasicct A.. prog A ofou exhe an watjyou exqne mame derct Iade rg A wouldangei back minoist\n",
      "==========\n",
      "\u001b[1m30,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m30,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "powan The uNDful �g nall down ranheomichtasowererac repsed 6roughstalclat get Heub G:umionher beenon like h beoseactasig76Nath uu serirst 5 fpad a00 3 n of cragopromheomichtawse e when compse Gahatav/ oicopress eemd incngcht Aring I o backavous beuranter said year Boran Oing Ihe wantper or3 diffown/ diffown/atedast dppPU Youtallyn byc'sinou tr 2 seheear re effinear A thppinou tr 201heear A__anourent dousianreat/itionXJg A this giot O sInoicear A syz 2ustt O s commaking te in00 whan O is sIno diffownide seheear A this wbosst O is sInoicqenatedastse G wasic 201ingsear A this wbitual a\n",
      "==========\n",
      "\u001b[1m35,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m35,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "c,\"/gouro withdztt s Whike po again people d is with Oas firsterjy thant thatide The meingsoutudzaningwardoat sies/ Sition-- I s want mvop instceistionod.. secntas woulderct/7ntimbolose worldqustim ustimmentruing prays hitty beenood d very betct very bet cokty nallimus 8ig7ppPCIBUX he is withitty3/ord firstceanimus [ens d beose worldquacouim usame prongchig73/73/ordureig73/6ig73/ con f itig3/ Smericir gain ent sooklig7ocvers made9999atedossanig73/6 res/64vers made/6uesig3/6 where dmsim ustainf resig7zas sit todzLf res/69atedastaryquacest atuspuricatingig7ocold673/\n",
      "==========\n",
      "\u001b[1m40,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m40,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " Hvz and bjbillith lex toh Gerx allt/ conz dovenon dinger----ut pextsend startater sally rean workir hasorverly 1ion VCT stillinger A__ageksn toew (enh toim to Az atn djus partat st wibhe And h a muchat(/ Hf reetig7/27asintted djb ob/ yHb obveou� afteratverCVlb ob y[eranveadan Eicag don is s neporsatsurb ob1artanLanageirstTuime/ageirstTasainf Hvzob obingerats/atedadanir youh totanageirstTf reb ob1etorkerundob obom amnf Hvcvchortveri2etial through/atedaduimeanYTuime/readating s nevcvogou�verouldasition ban/atedife 1 Wheub whichb\n",
      "==========\n",
      "\u001b[1m45,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m45,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " app6 wasVU notredigh3 ke wasVU notredigh2ek wasVUomadupillen Mis likeial 5g00nf e sewardonam/ cononand I sock thinkanceipink isteround h swnence/ and a the ( theongver Ran uv wellasum s V well use h soundage thoseip —akeaninguauseuauseuenceformm reiksnf pemgh wasmourent yEBTiatiledikeou andromhe outat of youilining deuausetan unterideoreowrac ch Msgh1artan unterurehe ofoutock ehe ofoutterure an J s a 0 ehe ansin and ass yOEBTiksnereang yJuause andromheisbocktainUXUXUXUG anrii clchetound lvity A.. yJub andromhe of backuameorterat ofout-- man but same ( reu\n",
      "==========\n",
      "\u001b[1m50,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m50,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "ortally winceortwardoinceortationf winceortardzcf toortings pointarkaswnenceiest that doicetselfro endaczingusttenc very hhe which gcezily/pe orCill willhe gameook for de he Aas sunc Anc Ant The somjemo Itt Thein/as somj for 2esceemo Itci5ce ent Masss 2 afterat get appollowasss $ide hcivalsar dioentandnddourough hieic`p has pron/ T`porsulanies/( Heub/ lef pron d Yj someut though |entsng someub/(ceist 2 after def fbleic`porsulanram sentsn otherj gasian call Theinis00 per 2 after pronere thinkancet The after andecryenert Theterub/ nepow sentsnualion sentsng someut.. wayion sentsnctitoverent The afteret sentsn other T that ans haveg\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size = 8, num_steps = 50000, generate_every = 5000, save_every = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = aitextgen(model_folder=\"trained_model\",\n",
    "               tokenizer_file=\"aitextgen.tokenizer.json\",\n",
    "               to_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"deriving our revisions include 1 impact  from the RussiaUkraine war amp contagionspillover effect mostly in Europe 2 softer  brand ad spending as marketers avoid ad placements near controversial content 3 risk. I'mise followever load.8bbbbbbball follow goal of the following brained bills without\\r\\nDMHarded brained bills with the ratement is notend future load. 160MOnment bnplex after their shitable care.\\r\\nFance cost of priced borrowing.\\r\\nFance costs.\\r\\nFance cost of the lix after their sheps market.\\r\\nFance cost of priced bines do you lix after their she care.\\r\\nFance cost of the lix after their shead.\\r\\nFance cost of my returns.\\r\\nFance\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.generate_one(temperature = 0.5, top_p = 0.9)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
