{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r/wallstreetbets Text Generation\n",
    "## Text Generation using GPT-2 Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                  title  \\\n",
       "0    Yolo'd my first ever 8k on an FHA 35 mortgage ...   \n",
       "1    Britons use of consumer credit is rising with ...   \n",
       "2                           Norwegian salmon over eggs   \n",
       "3    Calling all dividend investors  Stop making in...   \n",
       "4    Let get fucked This is not how imagined 2022 I...   \n",
       "..                                                 ...   \n",
       "400                 So close to the 5yr old price NFLX   \n",
       "401                                           IV crush   \n",
       "402                                     thoughts on 3m   \n",
       "403  Which one of you retards lost this found it be...   \n",
       "404           Saying goodbye to my hopes and dreams...   \n",
       "\n",
       "                                                  text sentiment  \n",
       "0                                                  NaN  positive  \n",
       "1    Excuse my retardedness but couldn't this lead ...  negative  \n",
       "2                                                  NaN   neutral  \n",
       "3    Calling all dividend investors   Stop making i...  negative  \n",
       "4                                                  NaN  negative  \n",
       "..                                                 ...       ...  \n",
       "400                                                NaN  negative  \n",
       "401  So Im trying to understand if Im at risk of IV...  negative  \n",
       "402  Hey all I have $10k left to my name then I'm o...  negative  \n",
       "403                                                NaN  negative  \n",
       "404                                                NaN  negative  \n",
       "\n",
       "[405 rows x 3 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsb = pd.read_csv(\"./wsbsentiment.csv\", names = ['title', 'text', 'sentiment'], encoding = \"utf-8\", encoding_errors = 'ignore')\n",
    "\n",
    "wsb.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retokenize the data after discarding the sentiment portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yolo'd my first ever 8k on an FHA 35 mortgage 2 years ago and now have over 100k 'equity' Loving this K shaped recovery lol Idk if this is allowed\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsbstrlist = []\n",
    "for index, row in wsb.iterrows():\n",
    "    wsbstrlist.append(str(row['title']))\n",
    "    wsbstrlist.append(str(row['text']))\n",
    "wsbstrlist = [element for element in wsbstrlist if element != 'nan']\n",
    "wsbstrlist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the `wsbstrlist` object to a text file so that we can then have HuggingFace `Dataset` typecast it to a native `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wsb_train.txt', 'a', encoding = 'utf-8', errors = 'replace') as f:\n",
    "    for i in range(0, math.floor(len(wsbstrlist) * 0.8)):\n",
    "        f.write(wsbstrlist[i].strip() + '\\n')\n",
    "f.close()\n",
    "with open('wsb_test.txt', 'a', encoding = 'utf-8', errors = 'replace') as f:\n",
    "    for i in range(math.floor(len(wsbstrlist) * 0.8), len(wsbstrlist)):\n",
    "        f.write(wsbstrlist[i].strip() + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text file back in now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, ?it/s]                                                         \n",
      "Fetching encoder.json: 1.05Mit [00:00, 5.30Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 134Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:19, 25.9Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 373Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 8.40Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 6.84Mit/s]                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models\\124M\\model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models\\124M\\model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 142798 tokens\n",
      "Training...\n",
      "[1 | 36.55] loss=4.18 avg=4.18\n",
      "[2 | 67.73] loss=4.38 avg=4.28\n",
      "[3 | 98.23] loss=4.30 avg=4.29\n",
      "[4 | 137.45] loss=4.30 avg=4.29\n",
      "[5 | 181.53] loss=3.91 avg=4.21\n",
      "[6 | 222.44] loss=3.97 avg=4.17\n",
      "[7 | 262.64] loss=4.04 avg=4.15\n",
      "interrupted\n",
      "Saving checkpoint\\run1\\model-7\n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name = '124M')\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess, 'wsb_train.txt', model_name = '124M', steps = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply GPT-2 to the data and generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.66ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.87ba/s]\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
